{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import blingfire as bf\n",
    "from __future__ import annotations\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import faiss\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.special import softmax\n",
    "deberta_v3_large = '/kaggle/input/108-12500/108-12500-0.91'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_openbook import get_contexts, generate_openbook_output\n",
    "import pickle\n",
    "\n",
    "get_contexts()\n",
    "generate_openbook_output()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitList(mylist, chunk_size):\n",
    "    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n",
    "\n",
    "def get_relevant_documents_parsed(df_valid):\n",
    "    df_chunk_size=600\n",
    "    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n",
    "    modified_texts = paraphs_parsed_dataset.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"title\"],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"text\"],\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_documents(df_valid):\n",
    "    df_chunk_size=2000\n",
    "    \n",
    "    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n",
    "    modified_texts = cohere_dataset_filtered.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         cohere_dataset_filtered[idx.item()][\"title\"],\n",
    "                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def retrieval(df_valid, modified_texts):\n",
    "    \n",
    "    corpus_df_valid = df_valid.apply(lambda row:\n",
    "                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n",
    "                                     axis=1).values\n",
    "    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words)\n",
    "    vectorizer1.fit(corpus_df_valid)\n",
    "    vocab_df_valid = vectorizer1.get_feature_names_out()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words,\n",
    "                                 vocabulary=vocab_df_valid)\n",
    "    vectorizer.fit(modified_texts[:500000])\n",
    "    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n",
    "    \n",
    "    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    chunk_size = 100000\n",
    "    top_per_chunk = 10\n",
    "    top_per_query = 10\n",
    "\n",
    "    all_chunk_top_indices = []\n",
    "    all_chunk_top_values = []\n",
    "\n",
    "    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n",
    "        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n",
    "        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n",
    "        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n",
    "        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n",
    "\n",
    "        all_chunk_top_indices.append(chunk_top_indices + idx)\n",
    "        all_chunk_top_values.append(chunk_top_values)\n",
    "\n",
    "    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n",
    "    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n",
    "    \n",
    "    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n",
    "    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n",
    "    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n",
    "    \n",
    "    return articles_indices, merged_top_scores\n",
    "\n",
    "\n",
    "def prepare_answering_input(\n",
    "        tokenizer, \n",
    "        question,  \n",
    "        options,   \n",
    "        context,   \n",
    "        max_seq_length=4096,\n",
    "    ):\n",
    "    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n",
    "    c_plus_q_4 = [c_plus_q] * len(options)\n",
    "    tokenized_examples = tokenizer(\n",
    "        c_plus_q_4, options,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"longest\",\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n",
    "    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n",
    "    example_encoded = {\n",
    "        \"input_ids\": input_ids.to(model.device.index),\n",
    "        \"attention_mask\": attention_mask.to(model.device.index),\n",
    "    }\n",
    "    return example_encoded\n",
    "\n",
    "\n",
    "stop_words = ['each', 'you', 'the', 'use', 'used',\n",
    "                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n",
    "                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n",
    "                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n",
    "                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n",
    "                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n",
    "                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n",
    "                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n",
    "                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n",
    "                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n",
    "                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n",
    "                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n",
    "                  'did', 'theirs', 'can', 'those',\n",
    "                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n",
    "                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n",
    "                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n",
    "                  'yours', 'but', 'being', \"wasn't\", 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n",
    "retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_articles = get_relevant_documents(df_valid)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(deberta_v3_large)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(deberta_v3_large).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "submit_ids = []\n",
    "\n",
    "for index in tqdm(range(df_valid.shape[0])):\n",
    "    columns = df_valid.iloc[index].values\n",
    "    submit_ids.append(columns[0])\n",
    "    question = columns[1]\n",
    "    options = [columns[2], columns[3], columns[4], columns[5], columns[6]]\n",
    "    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n",
    "    context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n",
    "    inputs1 = prepare_answering_input(\n",
    "        tokenizer=tokenizer, question=question,\n",
    "        options=options, context=context1,\n",
    "        )\n",
    "    inputs2 = prepare_answering_input(\n",
    "        tokenizer=tokenizer, question=question,\n",
    "        options=options, context=context2,\n",
    "        )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)    \n",
    "        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n",
    "        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs2 = model(**inputs2)\n",
    "        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n",
    "        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n",
    "        \n",
    "    probability_ = (probability1 + probability2)/2\n",
    "\n",
    "\n",
    "    predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n",
    "\n",
    "\n",
    "predictions = [\" \".join(i) for i in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'id':submit_ids,'prediction':predictions}).to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
